#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
@author: jeffreybruggeman
"""

import pandas as pd
import numpy as np
# Loading original data to preform knn and decision tree classifiers on unedited data (only mapped classes)
df = pd.read_csv("CancerData.csv", sep=',')
class_map = {label:value for value,label in enumerate(np.unique(df['Class']))}
df['Class'] = df['Class'].map(class_map)

# Split the data again
X, y = df.iloc[:,1:32], df['Class'].values
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.33 ,stratify=y, random_state=0)
# Saved the data generated by each knn iteration in kF
kF1= pd.DataFrame(columns=['Neighbors', 'Training Error', 'Testing Error'])
from sklearn.neighbors import KNeighborsClassifier
print('\nTest 1: KNN for different levels of neighbors using \nmanhattan distance and uniform weight on a mapped class set')
# Iterate knn for different neighbors 
for k in range(1,11):
    knn = KNeighborsClassifier(n_neighbors=k, p=1, metric='minkowski')
    knn.fit(X_train, y_train)
    knn.predict(X_test)
    scoreTrain = knn.score(X_train, y_train)
    scoreTest = knn.score(X_test, y_test)
    kF1.loc[k]=[k,1-scoreTrain,1-scoreTest]
# print data and separate next output    
print(kF1)

# Build decision tree and output results based on the data from above
print('--------------------------------------------------')
print('\nTest 2: Decision tree at different heights using \nentropy and a mapped class set')
from sklearn.tree import DecisionTreeClassifier
entropyModel = pd.DataFrame(columns=['Level Limit', 'Error Rate Training', 'Error Rate Testing'])
for height in range(1,11):
    dtree=DecisionTreeClassifier(criterion='entropy', max_depth=height, random_state=0)
    dtree=dtree.fit(X_train,y_train)
    dtree.predict(X_test)
    scoreTrain = dtree.score(X_train, y_train)
    scoreTest = dtree.score(X_test, y_test)
    entropyModel.loc[height] = [height, 1-scoreTrain, 1-scoreTest]
print(entropyModel)
print('--------------------------------------------------')

# clamp data using 2x standard deviation from the mean
for (name, Series) in df.iteritems():
    if (name=='Class'):
        continue
#        df[name] = df[name].map(class_map)
    else:
        mean=df[name].mean()
        std=np.std(df[name])
        df[name].clip(lower=mean-(2*std), upper=mean+(2*std), inplace=True)
# saving in case I want to use it later
df.to_csv('CancerDataClamped.csv', encoding='utf-8', index=False)

# Split the data again
X, y = df.iloc[:,1:32], df['Class'].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.33 ,stratify=y, random_state=0)
# Saved the data generated by each knn iteration in kF
kF2= pd.DataFrame(columns=['Neighbors', 'Training Error', 'Testing Error'])
print('\nTest 3: KNN for different levels of neighbors using manhattan \ndistance and uniform weight on a mapped class and clamped set')
# Iterate knn for different neighbors for clamped dataset
for k in range(1,11):
    knn = KNeighborsClassifier(n_neighbors=k, p=1, metric='minkowski')
    knn.fit(X_train, y_train)
    knn.predict(X_test)
    scoreTrain = knn.score(X_train, y_train)
    scoreTest = knn.score(X_test, y_test)
    kF2.loc[k]=[k,1-scoreTrain,1-scoreTest]
    
print(kF2)
print('--------------------------------------------------')

print('\nTest 4: Decision tree at different heights using \nentropy on a mapped class and clamped set')
# Running a decision tree with the clamped information
entropyModel = pd.DataFrame(columns=['Level Limit', 'Error Rate Training', 'Error Rate Testing'])
for height in range(1,11):
    dtree=DecisionTreeClassifier(criterion='entropy', max_depth=height, random_state=0)
    dtree=dtree.fit(X_train,y_train)
    dtree.predict(X_test)
    scoreTrain = dtree.score(X_train, y_train)
    scoreTest = dtree.score(X_test, y_test)
    entropyModel.loc[height] = [height, 1-scoreTrain, 1-scoreTest]

print(entropyModel)
print('--------------------------------------------------')

# loading normalized cander data from task 1
df=pd.read_csv("CancerNormalized.csv", sep=',')
# loading the ordered random forest impurity data from task 4
assessment =pd.read_csv('assessment.csv', sep=',')
# loading 3 attributes from normalized data: Class, and the top two attributes from 
# the random forest data. I originally loaded the top 6 or 7 and reduced it each time
# there was no change in accuracy, ending with simply building a tree that gets 95% 
# accuracy with a height of 2 using only 2 attributes
trimmed = pd.DataFrame(df[['Class',assessment.columns[0],assessment.columns[1]]])
X, y = trimmed.iloc[:,1:3], trimmed['Class'].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.33 ,stratify=y, random_state=0)
print('\nTest 5: Decision tree at different heights using \nentropy and random forest selected attributes on the normalized set')
entropyModel = pd.DataFrame(columns=['Level Limit', 'Error Rate Training', 'Error Rate Testing'])
for height in range(1,11):
    dtree=DecisionTreeClassifier(criterion='entropy', max_depth=height, random_state=0)
    dtree=dtree.fit(X_train,y_train)
    dtree.predict(X_test)
    scoreTrain = dtree.score(X_train, y_train)
    scoreTest = dtree.score(X_test, y_test)
    entropyModel.loc[height] = [height, 1-scoreTrain, 1-scoreTest]

# after looked at data I made a tree with graphviz for reference using the best classifier
dtree=DecisionTreeClassifier(criterion='entropy', max_depth=2, random_state=0)
dtree=dtree.fit(X_train,y_train)
from sklearn.tree import export_graphviz
export_graphviz(dtree, out_file='RFtree.dot')  
# print results
print(entropyModel)
print('--------------------------------------------------')

# use the random forest data above on a KNN classifier just to see what would happen
kF3= pd.DataFrame(columns=['Neighbors', 'Training Error', 'Testing Error'])
print('\nTest 6: KNN for different levels of neighbors using manhattan \ndistance and uniform weight on random forest selected \nattributes from the normalized set')
# Iterate knn for different neighbors for clamped normalized dataset
for k in range(1,11):
    knn = KNeighborsClassifier(n_neighbors=k, p=1, metric='minkowski')
    knn.fit(X_train, y_train)
    knn.predict(X_test)
    scoreTrain = knn.score(X_train, y_train)
    scoreTest = knn.score(X_test, y_test)
    kF3.loc[k]=[k,1-scoreTrain,1-scoreTest]
    
print(kF3)
print('--------------------------------------------------')

# load clamped data created above
clamped = pd.read_csv('CancerDataClamped.csv', sep=',')
# Created a SPLOM to look at the data created by this again, commented out
# so it wouldn't create everytime I run this
# =============================================================================
# import seaborn as sns
# sns.set(style='ticks')
# sns.pairplot(clamped, hue='Class')
# =============================================================================

for (name, Series) in clamped.iteritems():
    if (name=='Class'):
        continue
    else:
        # normalize data on range 0,2
        col_min=clamped[name].min()
        col_max=clamped[name].max()
        for x in range(int(len(df[name]))):
            clamped[name].at[x]=( (clamped[name].at[x]-col_min)/(col_max-col_min) ) * 2

# Based on splom, make a new dataset of generated features to test with knn
# Choosing concave points3 and the 3 textures because the spread of data points
# in the SPLOM seems to be the most well divided
print(clamped)
df_gen=pd.DataFrame()
df_gen['Class']=clamped['Class']
df_gen['con3*tex1'] = clamped['concave points3']*clamped['texture1']
df_gen['con3*tex2'] = clamped['concave points3']*clamped['texture2']
X, y = df_gen.iloc[:,1:3], df_gen['Class'].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.33 ,stratify=y, random_state=0)
kF4= pd.DataFrame(columns=['Neighbors', 'Training Error', 'Testing Error'])

print('\nTest 7: KNN for different levels of neighbors using manhattan \ndistance and uniform weight on a generated set')
# Iterate knn for different neighbors for clamped normalized generated dataset
for k in range(1,11):
    knn = KNeighborsClassifier(n_neighbors=k, p=1, metric='minkowski')
    knn.fit(X_train, y_train)
    knn.predict(X_test)
    scoreTrain = knn.score(X_train, y_train)
    scoreTest = knn.score(X_test, y_test)
    kF4.loc[k]=[k,1-scoreTrain,1-scoreTest]
    
print(kF4)
print('--------------------------------------------------')